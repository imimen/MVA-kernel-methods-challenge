{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from models import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# global variables\n",
    "DATA_FOLDER = \"machine-learning-with-kernel-methods-2021\"\n",
    "TRAIN_FILE = \"Xtr{}.csv\"\n",
    "LABEL_FILE = \"Ytr{}.csv\"\n",
    "TEST_FILE = \"Xte{}.csv\"\n",
    "N = 3  # number of datasets\n",
    "\n",
    "HIST_FILE = \"results-history.txt\"\n",
    "\n",
    "baselines = {\n",
    "    \"ridge\": kernelRidge,\n",
    "    \"svm\": kernelSVM,\n",
    "    \"logistic\": KernelLogistic,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_instance(args):\n",
    "    \"\"\"\n",
    "    This returns the results on the tests sets of the model corresponding to args\n",
    "    \"\"\"\n",
    "\n",
    "    feature_type = args['features'].lower()\n",
    "    fmaker = feature_extractor(\n",
    "        feature_type=feature_type,\n",
    "        k=args['k'],\n",
    "        m=args['m'],\n",
    "        lmbda=args['lmbda'],\n",
    "        order_of_fourier_kmers=args['order_of_fourier_kmers'],\n",
    "        nb_of_fourier_coeffs=args['order_of_fourier_kmers'],\n",
    "    )\n",
    "\n",
    "    kernel_type = args['kernel'].lower() #if feature_type == \"bow\" else \"linear\"\n",
    "    \n",
    "    clf = Baseline(\n",
    "        baseline_type=args['baseline'].lower(), \n",
    "        kernel_type=kernel_type, \n",
    "        d=args['d'], \n",
    "        sigma=args['sigma'], \n",
    "        c=args['c'])\n",
    "    \n",
    "    index = []\n",
    "    pred = []\n",
    "    accs = []\n",
    "    pred_val = []\n",
    "    true_val = []\n",
    "    for i in range(N):\n",
    "        xtrain, ytrain, xte, ids = fmaker(i)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(xtrain)\n",
    "        xtrain = scaler.transform(xtrain)\n",
    "        xte = scaler.transform(xte)\n",
    "        xtr, xval, ytr, yval = train_test_split(\n",
    "            xtrain, ytrain, test_size=0.3, random_state=42\n",
    "        )\n",
    "\n",
    "        _ = clf.fit(xtr, ytr)\n",
    "        predval = clf.predict(xtr, xval)\n",
    "        print(confusion_matrix(yval, predval))\n",
    "        \n",
    "        acc = accuracy_score(yval, predval)\n",
    "        print(\"accuracy is \" + str(acc))\n",
    "        ytest = clf.predict(xtr, xte)\n",
    "\n",
    "        index.extend(ids)\n",
    "        pred.extend(ytest)\n",
    "        accs.append(acc)\n",
    "        pred_val.append(predval)\n",
    "        true_val.extend(yval)\n",
    "        \n",
    "    # name = args['features'].lower() + \"_k_\" + str(args['k']) + clf.type\n",
    "    # save_file(index, pred, name=name)\n",
    "    # save_results(name, accs, sum(accs)/N)\n",
    "    \n",
    "    return index, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
